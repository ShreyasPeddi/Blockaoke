{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "def create_siamese_network(input_shape):\n",
        "    image_input = Input(shape=input_shape)\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu')(conv1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu')(conv2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu')(conv3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    flatten = Flatten()(conv4)\n",
        "\n",
        "    fc1 = Dense(256, activation='relu')(flatten)\n",
        "    fc1 = Dropout(0.2)(fc1)\n",
        "\n",
        "    fc2 = Dense(128, activation='relu')(fc1)\n",
        "    fc2 = Dropout(0.1)(fc2)\n",
        "\n",
        "    fc3 = Dense(64, activation='relu')(fc2)\n",
        "\n",
        "    model = Model(inputs=image_input, outputs=fc3)\n",
        "\n",
        "    return model\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    y_true = K.cast(y_true, dtype=tf.float32)  # Cast y_true to float32\n",
        "    return K.mean(\n",
        "        y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0))\n",
        "    )\n",
        "\n",
        "\n",
        "def euclidean_distance(vectors):\n",
        "    x, y = vectors\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "\n",
        "def create_siamese_network_model(input_shape):\n",
        "    siamese_network = create_siamese_network(input_shape)\n",
        "    input_image_1 = Input(shape=input_shape)\n",
        "    input_image_2 = Input(shape=input_shape)\n",
        "    output_1 = siamese_network(input_image_1)\n",
        "    output_2 = siamese_network(input_image_2)\n",
        "\n",
        "    distance = Lambda(euclidean_distance)([output_1, output_2])\n",
        "\n",
        "    model = Model(inputs=[input_image_1, input_image_2], outputs=distance)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set the input shape of the images\n",
        "input_shape = (64, 64, 3)\n",
        "\n",
        "# Create the Siamese network model\n",
        "siamese_model = create_siamese_network_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "siamese_model.compile(optimizer='adam', loss=contrastive_loss)\n",
        "\n",
        "# Paths to directories containing similar and dissimilar image pairs\n",
        "similar_image_dir = '/content/drive/MyDrive/Siamese_Dataset/similar'\n",
        "dissimilar_image_dir = '/content/drive/MyDrive/Siamese_Dataset/dissimilar'\n",
        "\n",
        "# Get the list of filenames in each directory\n",
        "similar_image_files = [f for f in os.listdir(similar_image_dir) if os.path.isfile(os.path.join(similar_image_dir, f))]\n",
        "dissimilar_image_files = [f for f in os.listdir(dissimilar_image_dir) if os.path.isfile(os.path.join(dissimilar_image_dir, f))]\n",
        "\n",
        "# Shuffle the lists of filenames\n",
        "random.shuffle(similar_image_files)\n",
        "random.shuffle(dissimilar_image_files)\n",
        "\n",
        "# Define the number of similar and dissimilar pairs to use for training\n",
        "num_pairs = min(len(similar_image_files), len(dissimilar_image_files))\n",
        "num_training_pairs = min(int(0.8 * num_pairs), len(similar_image_files) - 1, len(dissimilar_image_files) - 1)\n",
        "\n",
        "# Initialize the lists for training image pairs and labels\n",
        "training_image_pairs = []\n",
        "training_labels = []\n",
        "\n",
        "# Generate training data with similar image pairs\n",
        "for i in range(num_training_pairs):\n",
        "    # Select a random pair of similar images\n",
        "    similar_image_1 = similar_image_files[i]\n",
        "    similar_image_2 = similar_image_files[i + 1]\n",
        "\n",
        "    # Add the image pair and label to the training lists\n",
        "    training_image_pairs.append([\n",
        "        os.path.join(similar_image_dir, similar_image_1),\n",
        "        os.path.join(similar_image_dir, similar_image_2)\n",
        "    ])\n",
        "    training_labels.append(1)  # Label as similar\n",
        "\n",
        "# Generate training data with dissimilar image pairs\n",
        "for i in range(num_training_pairs):\n",
        "    # Select a random pair of dissimilar images\n",
        "    dissimilar_image_1 = dissimilar_image_files[i]\n",
        "    dissimilar_image_2 = dissimilar_image_files[i + 1]\n",
        "\n",
        "    # Add the image pair and label to the training lists\n",
        "    training_image_pairs.append([\n",
        "        os.path.join(dissimilar_image_dir, dissimilar_image_1),\n",
        "        os.path.join(dissimilar_image_dir, dissimilar_image_2)\n",
        "    ])\n",
        "    training_labels.append(0)  # Label as dissimilar\n",
        "\n",
        "# Convert the training image pairs and labels to numpy arrays\n",
        "training_image_pairs = np.array(training_image_pairs)\n",
        "training_labels = np.array(training_labels)\n",
        "\n",
        "# Fit the model on the training data\n",
        "siamese_model.fit(\n",
        "    [np.array([img_to_array(load_img(pair[0], target_size=input_shape[:2])) / 255.0 for pair in training_image_pairs if os.path.isfile(pair[0])]),\n",
        "     np.array([img_to_array(load_img(pair[1], target_size=input_shape[:2])) / 255.0 for pair in training_image_pairs if os.path.isfile(pair[1])])],\n",
        "    training_labels,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "siamese_model.save('siamese_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n3SUOKqSRTf",
        "outputId": "d9c42883-99ab-4b83-c2be-47dde6d5b61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 12s 12s/step - loss: 16.9431\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 9s 9s/step - loss: 110613.2734\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 7s 7s/step - loss: 82735.5312\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 8s 8s/step - loss: 24420.7480\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 7s 7s/step - loss: 22875.4785\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 8s 8s/step - loss: 19745.2969\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 8s 8s/step - loss: 21122.9902\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 8s 8s/step - loss: 8278.6621\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 9s 9s/step - loss: 28387.9316\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 7s 7s/step - loss: 18986.7949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    y_true = K.cast(y_true, dtype=tf.float32)  # Cast y_true to float32\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "# Define the custom_objects dictionary with the contrastive_loss function\n",
        "custom_objects = {'contrastive_loss': contrastive_loss}\n",
        "\n",
        "# Load the saved model with the custom loss function\n",
        "siamese_model = load_model('siamese_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "# Set the input shape of the images\n",
        "input_shape = (64, 64, 3)\n",
        "\n",
        "# Load and preprocess two image files\n",
        "image_file_1 = '/content/drive/MyDrive/Siamese_Dataset/similar/edsheeran_cover.jpg'\n",
        "image_file_2 = '/content/drive/MyDrive/Siamese_Dataset/dissimilar/badCover.jpg'\n",
        "image_1 = img_to_array(load_img(image_file_1, target_size=input_shape[:2])) / 255.0\n",
        "image_2 = img_to_array(load_img(image_file_2, target_size=input_shape[:2])) / 255.0\n",
        "image_1 = np.expand_dims(image_1, axis=0)\n",
        "image_2 = np.expand_dims(image_2, axis=0)\n",
        "\n",
        "# Predict the distance between the two images\n",
        "distance = siamese_model.predict([image_1, image_2])\n",
        "print('Distance:', distance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXigcC55YTLh",
        "outputId": "3442a7d9-0d6b-4abd-b5b5-107bd7e40945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 413ms/step\n",
            "Distance: [[2.696117]]\n"
          ]
        }
      ]
    }
  ]
}